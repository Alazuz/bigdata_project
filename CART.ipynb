{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d63500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger vos données journalières\n",
    "data = pd.read_csv('data\\df.csv',sep='|')\n",
    "ts = data['close']  # votre série temporelle journalière\n",
    "\n",
    "# Visualisation rapide\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ts)\n",
    "plt.title('Prix journaliers du S&P500')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63937e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithme CART\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SP500CARTForecaster:\n",
    "    \"\"\"\n",
    "    Modèle CART pour prédiction du S&P 500 avec données temporelles\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prediction_type='regression'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        prediction_type : str\n",
    "            'regression' pour prédire le prix\n",
    "            'classification' pour prédire la direction (hausse/baisse)\n",
    "        \"\"\"\n",
    "        self.prediction_type = prediction_type\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "        self.features = None\n",
    "        self.target = None\n",
    "        self.feature_names = []\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Charge les données depuis le fichier Excel\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            print(f\"Fichier chargé: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
    "            \n",
    "            # Détecter la colonne de date\n",
    "            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            if date_cols:\n",
    "                df[date_cols[0]] = pd.to_datetime(df[date_cols[0]])\n",
    "                df.set_index(date_cols[0], inplace=True)\n",
    "                print(f\"Colonne de date détectée: {date_cols[0]}\")\n",
    "            \n",
    "            self.data = df\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_time_series_features(self, target_column='close', lookback_periods=20):\n",
    "        \"\"\"\n",
    "        Crée des features temporelles adaptées aux arbres de décision\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_column : str\n",
    "            Colonne cible (prix S&P 500)\n",
    "        lookback_periods : int\n",
    "            Nombre de périodes passées à utiliser comme features\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Données non chargées.\")\n",
    "            \n",
    "        df = self.data.copy()\n",
    "        \n",
    "        print(f\"Création des features temporelles (lookback: {lookback_periods} jours)...\")\n",
    "        \n",
    "        # === 1. FEATURES DE PRIX HISTORIQUES (LAG FEATURES) ===\n",
    "        \n",
    "        for i in range(1, lookback_periods + 1):\n",
    "            df[f'close_lag_{i}'] = df[target_column].shift(i)\n",
    "           \n",
    "        # === 2. FEATURES DE RENDEMENTS ===\n",
    "        \n",
    "        df['return_1d'] = df[target_column].pct_change()\n",
    "        for i in range(2, 11):  # Rendements sur 2 à 10 jours\n",
    "            df[f'return_{i}d'] = df[target_column].pct_change(periods=i)\n",
    "            \n",
    "        # Rendements cumulés\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'cumret_{window}d'] = df['return_1d'].rolling(window).sum()\n",
    "            \n",
    "        # === 3. FEATURES DE VOLATILITÉ ===\n",
    "        \n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'volatility_{window}d'] = df['return_1d'].rolling(window).std()\n",
    "            df[f'vol_ratio_{window}d'] = df[f'volatility_{window}d'] / df['volatility_20d']\n",
    "        \n",
    "        # === 4. FEATURES TECHNIQUES ===\n",
    "        \n",
    "        if all(col in df.columns for col in ['Open', 'High', 'Low', 'close']):\n",
    "            # Range et gaps\n",
    "            df['daily_range'] = (df['High'] - df['Low']) / df['close']\n",
    "            df['gap'] = (df['Open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "            df['body'] = (df['close'] - df['Open']) / df['close']\n",
    "            \n",
    "            # Patterns de chandelles (simplifiés)\n",
    "            df['green_candle'] = (df['close'] > df['Open']).astype(int)\n",
    "            df['big_move'] = (np.abs(df['return_1d']) > 0.02).astype(int)\n",
    "            df['gap_up'] = (df['gap'] > 0.005).astype(int)\n",
    "            df['gap_down'] = (df['gap'] < -0.005).astype(int)\n",
    "        \n",
    "        # RSI (approximatif)\n",
    "        delta = df[target_column].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n",
    "        df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n",
    "        \n",
    "        # === 6. FEATURES DE VOLUME (si disponible) ===\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            df['volume_ma_20'] = df['volume'].rolling(20).mean()\n",
    "            df['volume_ratio'] = df['volume'] / df['volume_ma_20']\n",
    "            df['high_volume'] = (df['volume_ratio'] > 1.5).astype(int)\n",
    "            df['low_volume'] = (df['volume_ratio'] < 0.5).astype(int)\n",
    "        \n",
    "        # === 7. FEATURES D'AUTRES VARIABLES (VIX, etc.) ===\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col not in [target_column, 'Open', 'High', 'Low', 'close', 'volume'] and df[col].dtype in ['float64', 'int64']:\n",
    "                # Lags des autres variables\n",
    "                for lag in [1, 2, 5]:\n",
    "                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                \n",
    "                # Changements\n",
    "                df[f'{col}_change'] = df[col].diff()\n",
    "                df[f'{col}_pct_change'] = df[col].pct_change()\n",
    "        \n",
    "        # === 8. FEATURES COMBINÉES ===\n",
    "        \n",
    "        # Momentum combinations\n",
    "        df['price_momentum'] = (df['return_1d'] > 0).astype(int) + (df['return_5d'] > 0).astype(int)\n",
    "        df['vol_momentum'] = (df['volatility_5d'] > df['volatility_20d']).astype(int)\n",
    "        \n",
    "        # Regime features\n",
    "        df['bull_market'] = (df[f'ma_50'] > df[f'ma_200']).astype(int)\n",
    "        df['above_ma_20'] = (df[target_column] > df['ma_20']).astype(int)\n",
    "        \n",
    "        self.data = df\n",
    "        print(f\"Features créées. Nouvelles dimensions: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_cart_data(self, target_column='close', forecast_horizon=1):\n",
    "        \"\"\"\n",
    "        Prépare les données pour CART (X, y)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_column : str\n",
    "            Variable à prédire\n",
    "        forecast_horizon : int\n",
    "            Nombre de jours à l'avance à prédire\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Données non préparées.\")\n",
    "            \n",
    "        # Créer la variable cible\n",
    "        if self.prediction_type == 'regression':\n",
    "            # Prédire le prix futur\n",
    "            self.data['target'] = self.data[target_column].shift(-forecast_horizon)\n",
    "            print(f\"Target: Prix du S&P 500 dans {forecast_horizon} jour(s)\")\n",
    "            \n",
    "        elif self.prediction_type == 'classification':\n",
    "            # Prédire la direction (hausse/baisse)\n",
    "            future_return = self.data[target_column].shift(-forecast_horizon).pct_change()\n",
    "            self.data['target'] = (future_return > 0).astype(int)\n",
    "            print(f\"Target: Direction du S&P 500 dans {forecast_horizon} jour(s) (0=Baisse, 1=Hausse)\")\n",
    "        \n",
    "        # Sélectionner les features (exclure les variables non-prédictives)\n",
    "        exclude_cols = [target_column, 'target', 'Open', 'High', 'Low', 'close', 'Volume'] + \\\n",
    "                      [col for col in self.data.columns if \n",
    "                       'return' in col or 'change' in col or self.data[col].isna().sum() > len(self.data) * 0.1]\n",
    "        \n",
    "        feature_cols = [col for col in self.data.columns \n",
    "                       if col not in exclude_cols and self.data[col].dtype in ['int64', 'float64', 'bool']]\n",
    "        \n",
    "        # Préparer X et y\n",
    "        complete_data = self.data[feature_cols + ['target']].dropna()\n",
    "        \n",
    "        X = complete_data[feature_cols]\n",
    "        y = complete_data['target']\n",
    "        \n",
    "        self.features = X\n",
    "        self.target = y\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        print(f\"Dataset CART préparé:\")\n",
    "        print(f\"  - Observations: {len(X)}\")\n",
    "        print(f\"  - Features: {len(feature_cols)}\")\n",
    "        print(f\"  - Type de prédiction: {self.prediction_type}\")\n",
    "        \n",
    "        if self.prediction_type == 'classification':\n",
    "            print(f\"  - Distribution des classes: {y.value_counts().to_dict()}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def optimize_cart_parameters(self):\n",
    "        \"\"\"\n",
    "        Optimise les hyperparamètres CART avec validation temporelle\n",
    "        \"\"\"\n",
    "        if self.features is None or self.target is None:\n",
    "            raise ValueError(\"Données non préparées pour CART.\")\n",
    "            \n",
    "        print(\"Optimisation des hyperparamètres CART...\")\n",
    "        \n",
    "        # Paramètres à tester\n",
    "        if self.prediction_type == 'regression':\n",
    "            param_grid = {\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [20, 50, 100, 200],\n",
    "                'min_samples_leaf': [10, 20, 50],\n",
    "                'max_features': [0.3, 0.5, 0.7, 1.0]\n",
    "            }\n",
    "            model = DecisionTreeRegressor(random_state=42)\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "            \n",
    "        else:  # classification\n",
    "            param_grid = {\n",
    "                'max_depth': [5, 10, 15, 20],\n",
    "                'min_samples_split': [20, 50, 100],\n",
    "                'min_samples_leaf': [10, 20, 50],\n",
    "                'max_features': [0.3, 0.5, 0.7, 1.0],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "            model = DecisionTreeClassifier(random_state=42)\n",
    "            scoring = 'accuracy'\n",
    "        \n",
    "        # Validation croisée temporelle\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=tscv, scoring=scoring,\n",
    "            n_jobs=-1, verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(self.features, self.target)\n",
    "        \n",
    "        self.model = grid_search.best_estimator_\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "        print(f\"Score de validation: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    def fit_simple_cart(self, max_depth=10, min_samples_split=50, min_samples_leaf=20):\n",
    "        \"\"\"\n",
    "        Ajuste un modèle CART simple sans optimisation\n",
    "        \"\"\"\n",
    "        if self.features is None or self.target is None:\n",
    "            raise ValueError(\"Données non préparées.\")\n",
    "            \n",
    "        if self.prediction_type == 'regression':\n",
    "            self.model = DecisionTreeRegressor(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            self.model = DecisionTreeClassifier(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        self.model.fit(self.features, self.target)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(f\"Modèle CART {self.prediction_type} ajusté.\")\n",
    "        print(f\"Profondeur réelle de l'arbre: {self.model.tree_.max_depth}\")\n",
    "        print(f\"Nombre de feuilles: {self.model.tree_.n_leaves}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def feature_importance(self, top_n=15):\n",
    "        \"\"\"\n",
    "        Analyse l'importance des variables dans l'arbre CART\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Modèle non ajusté.\")\n",
    "            \n",
    "        # Récupérer les importances\n",
    "        importances = self.model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"=== TOP {top_n} FEATURES IMPORTANTES ===\")\n",
    "        print(feature_importance_df.head(top_n).to_string(index=False))\n",
    "        \n",
    "        # Graphique\n",
    "        top_features = feature_importance_df.head(top_n)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top {top_n} Variables - Importance dans l\\'Arbre CART')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance_df\n",
    "    \n",
    "    def validate_model(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Valide le modèle avec un split temporel\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Modèle non ajusté.\")\n",
    "            \n",
    "        # Split temporel (les dernières données pour le test)\n",
    "        split_idx = int(len(self.features) * (1 - test_size))\n",
    "        \n",
    "        X_train = self.features.iloc[:split_idx]\n",
    "        X_test = self.features.iloc[split_idx:]\n",
    "        y_train = self.target.iloc[:split_idx]\n",
    "        y_test = self.target.iloc[split_idx:]\n",
    "        \n",
    "        # Prédictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        print(f\"=== VALIDATION SUR {len(X_test)} OBSERVATIONS ===\")\n",
    "        \n",
    "        if self.prediction_type == 'regression':\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "            \n",
    "            print(f\"MAE: {mae:.2f}\")\n",
    "            print(f\"RMSE: {rmse:.2f}\")\n",
    "            print(f\"MAPE: {mape:.2f}%\")\n",
    "            \n",
    "            # Graphique régression\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "            plt.xlabel('Valeurs réelles')\n",
    "            plt.ylabel('Prédictions CART')\n",
    "            plt.title('Prédictions vs Réalité')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            recent_test = y_test.tail(50)\n",
    "            recent_pred = pd.Series(y_pred, index=y_test.index).tail(50)\n",
    "            plt.plot(recent_test.index, recent_test, label='Réel', linewidth=2)\n",
    "            plt.plot(recent_pred.index, recent_pred, label='CART', linestyle='--', linewidth=2)\n",
    "            plt.legend()\n",
    "            plt.title('50 dernières prédictions')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return {'mae': mae, 'rmse': rmse, 'mape': mape}\n",
    "            \n",
    "        else:  # classification\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(\"\\nRapport de classification:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # Matrice de confusion\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            plt.title('Matrice de Confusion - CART')\n",
    "            plt.colorbar()\n",
    "            tick_marks = [0, 1]\n",
    "            plt.xticks(tick_marks, ['Baisse', 'Hausse'])\n",
    "            plt.yticks(tick_marks, ['Baisse', 'Hausse'])\n",
    "            plt.ylabel('Classe réelle')\n",
    "            plt.xlabel('Classe prédite')\n",
    "            \n",
    "            # Ajouter les valeurs dans les cellules\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return {'accuracy': accuracy}\n",
    "    \n",
    "    def visualize_tree(self, max_depth_display=3):\n",
    "        \"\"\"\n",
    "        Visualise l'arbre de décision (limité en profondeur pour lisibilité)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Modèle non ajusté.\")\n",
    "            \n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Limiter la profondeur pour l'affichage\n",
    "        feature_names_short = [name[:15] + '...' if len(name) > 15 else name \n",
    "                              for name in self.feature_names]\n",
    "        \n",
    "        plot_tree(self.model, \n",
    "                 max_depth=max_depth_display,\n",
    "                 feature_names=feature_names_short,\n",
    "                 filled=True, \n",
    "                 rounded=True,\n",
    "                 fontsize=10)\n",
    "        \n",
    "        plt.title(f'Arbre CART S&P 500 (profondeur affichée: {max_depth_display})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Version texte pour les règles\n",
    "        tree_rules = export_text(self.model, \n",
    "                                feature_names=self.feature_names,\n",
    "                                max_depth=max_depth_display)\n",
    "        print(f\"\\n=== RÈGLES DE L'ARBRE (profondeur {max_depth_display}) ===\")\n",
    "        print(tree_rules[:2000] + \"...\" if len(tree_rules) > 2000 else tree_rules)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=== CART POUR S&P 500 FORECASTING ===\\n\")\n",
    "    \n",
    "    # Créer des données d'exemple\n",
    "    dates = pd.bdate_range('2020-01-01', '2024-12-31', freq='B')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simuler S&P 500 avec patterns réalistes\n",
    "    n_days = len(dates)\n",
    "    prices = [3000]  # Prix initial\n",
    "    \n",
    "    for i in range(1, n_days):\n",
    "        # Trend + noise + day-of-week effect\n",
    "        trend = 0.0003  # ~8% annuel\n",
    "        dow_effect = 0.001 if dates[i].weekday() == 4 else -0.0005 if dates[i].weekday() == 0 else 0\n",
    "        noise = np.random.normal(0, 0.015)\n",
    "        \n",
    "        change = trend + dow_effect + noise\n",
    "        new_price = prices[-1] * (1 + change)\n",
    "        prices.append(new_price)\n",
    "    \n",
    "    # Créer le DataFrame complet\n",
    "    demo_data = pd.DataFrame({\n",
    "        'close': prices,\n",
    "        'Open': [p * (1 + np.random.normal(0, 0.002)) for p in prices],\n",
    "        'High': [p * (1 + abs(np.random.normal(0, 0.008))) for p in prices],\n",
    "        'Low': [p * (1 - abs(np.random.normal(0, 0.008))) for p in prices],\n",
    "        'Volume': np.random.lognormal(15, 0.3, len(dates)),\n",
    "        'VIX': np.maximum(8, 20 + np.random.normal(0, 8, len(dates)))\n",
    "    }, index=dates)\n",
    "    \n",
    "    # === EXEMPLE 1: CART RÉGRESSION ===\n",
    "    print(\"1. CART RÉGRESSION (prédiction du prix)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cart_reg = SP500CARTForecaster(prediction_type='regression')\n",
    "    cart_reg.data = demo_data\n",
    "    \n",
    "    # Créer les features\n",
    "    cart_reg.create_time_series_features(target_column='close', lookback_periods=10)\n",
    "    \n",
    "    # Préparer pour CART\n",
    "    X, y = cart_reg.prepare_cart_data(target_column='close', forecast_horizon=1)\n",
    "    \n",
    "    # Ajuster modèle simple\n",
    "    cart_reg.fit_simple_cart(max_depth=10, min_samples_split=50)\n",
    "    \n",
    "    # Analyser\n",
    "    importance_reg = cart_reg.feature_importance(top_n=10)\n",
    "    metrics_reg = cart_reg.validate_model()\n",
    "    \n",
    "    # === EXEMPLE 2: CART CLASSIFICATION ===\n",
    "    print(\"\\n2. CART CLASSIFICATION (prédiction de direction)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    cart_clf = SP500CARTForecaster(prediction_type='classification')\n",
    "    cart_clf.data = demo_data\n",
    "    \n",
    "    # Créer les features\n",
    "    cart_clf.create_time_series_features(target_column='close', lookback_periods=10)\n",
    "    \n",
    "    # Préparer pour CART\n",
    "    X_clf, y_clf = cart_clf.prepare_cart_data(target_column='close', forecast_horizon=1)\n",
    "    \n",
    "    # Ajuster modèle simple\n",
    "    cart_clf.fit_simple_cart(max_depth=8, min_samples_split=30)\n",
    "    \n",
    "    # Analyser\n",
    "    importance_clf = cart_clf.feature_importance(top_n=10)\n",
    "    metrics_clf = cart_clf.validate_model()\n",
    "    \n",
    "    # Visualiser un des arbres\n",
    "    print(\"\\n3. VISUALISATION DE L'ARBRE\")\n",
    "    print(\"-\" * 35)\n",
    "    cart_clf.visualize_tree(max_depth_display=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
